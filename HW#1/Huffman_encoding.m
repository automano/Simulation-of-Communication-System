clear;
symbols = [1:66]; % Distinct symbols that data source can produce
p = [
0.000735474
0.000490316
0.000245158
0.000245158
0.000245158
0.000245158
0.000245158
0.159107624
0.004167688
0.009806325
0.001716107
0.001961265
0.000735474
0.000735474
0.007354744
0.003187056
0.007354744
0.001470949
0.001470949
0.068889434
0.001716107
0.012993381
0.001716107
0.02623192
0.000490316
0.028438343
0.000735474
0.098308409
0.000245158
0.010541799
0.000735474
0.019122334
0.002451581
0.037264035
0.001225791
0.056876685
0.001225791
0.000735474
0.001470949
0.004167688
0.000980633
0.024515813
0.001470949
0.018632018
0.001470949
0.056141211
0.001225791
0.053444472
0.000245158
0.014219171
0.000735474
0.000490316
0.047315519
0.000735474
0.055896053
0.001716107
0.067908801
0.004658004
0.021328757
0.010786958
0.000490316
0.01838686
0.001225791
0.001961265
0.013728855
0.003187056]; % Probability distribution
[dict,avglen] = huffmandict(symbols,p); % Create dictionary.
Entropy = p'*log2(p.^(-1));%Get the entropy of this article


